{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(fileDirectory, X, Y, label):\n",
    "    source = open(fileDirectory, 'r')\n",
    "    data = source.readlines()\n",
    "    source.close()\n",
    "    for i in range(1, len(data)-1):\n",
    "        raw = data[i][1:len(data[i])-2]\n",
    "        processed = raw.split(\",\")\n",
    "        Y.append(label)\n",
    "        X.append([float(num) for num in processed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Reading claire_hair_d1_102.txt\n",
      "Reading hair1_RUSDI_50.txt\n",
      "Reading hair2_RUSDI_50.txt\n",
      "Reading hair50_1_JN.txt\n",
      "Reading hair50_2_JN.txt\n",
      "Reading nic_hair_100.txt\n",
      "Reading claire_rocket_d1_32.txt\n",
      "Reading claire_rocket_d2_32.txt\n",
      "Reading claire_rocket_d3_22.txt\n",
      "Reading claire_rocket_d4_22.txt\n",
      "Reading nic_rocket_1_50.txt\n",
      "Reading nic_rocket_2_50.txt\n",
      "Reading rocket1_Rusdi_50.txt\n",
      "Reading rocket2_Rusdi_50.txt\n",
      "Reading rocket50_1_JN.txt\n",
      "Reading rocket50_2_JN.txt\n",
      "Reading claire_zigzag_d1_32.txt\n",
      "Reading claire_zigzag_d2_32.txt\n",
      "Reading claire_zigzag_d3_22.txt\n",
      "Reading claire_zigzag_d4_22.txt\n",
      "Reading nic_zigzag_100.txt\n",
      "Reading zigzag1_Rusdi_50.txt\n",
      "Reading zigzag_100_JN.txt\n",
      "Reading claire_elbowlock_d1_72.txt\n",
      "Reading claire_elbowlock_d2_32.txt\n",
      "Reading elbowLock_50_JN_1.txt\n",
      "Reading elbowLock_50_JN_2.txt\n",
      "Reading elbow_RUSDI_50.txt\n",
      "Reading nic_elbowlock_100.txt\n",
      "Reading claire_pushback_d1_52.txt\n",
      "Reading claire_pushback_d2_52.txt\n",
      "Reading nic_pushback_100.txt\n",
      "Reading pushback1_50_Rusdi.txt\n",
      "Reading pushback2_50_Rusdi.txt\n",
      "Reading pushback50_1_JN.txt\n",
      "Reading pushback50_2_JN.txt\n",
      "Reading claire_scarecrow_d1_22.txt\n",
      "Reading claire_scarecrow_d2_22.txt\n",
      "Reading claire_scarecrow_d3_22.txt\n",
      "Reading claire_scarecrow_d4_32.txt\n",
      "Reading claire_scarecrow_d5_13.txt\n",
      "Reading nic_scarecrow_1_50.txt\n",
      "Reading nic_scarecrow_2_50.txt\n",
      "Reading scarecrow1_50_Rusdi_1.txt\n",
      "Reading scarecrow_20_JN_2.txt\n",
      "Reading scarecrow_20_JN_4.txt\n",
      "Reading scarecrow_30_JN_1.txt\n",
      "Reading scarecrow_30_JN_3.txt\n",
      "Reading claire_shouldershrug_d1_72.txt\n",
      "Reading claire_shouldershrug_d2_32.txt\n",
      "Reading nic_shoudershurg_100.txt\n",
      "Reading shoulder1_Rusdi.txt\n",
      "Reading shoulder_JN_100.txt\n",
      "Reading claire_windowwipe_d1_32.txt\n",
      "Reading claire_windowwipe_d2_32.txt\n",
      "Reading claire_windowwipe_d3_22.txt\n",
      "Reading claire_windowwipe_d4_22.txt\n",
      "Reading nic_window_100.txt\n",
      "Reading window1_50_Rusdi.txt\n",
      "Reading window_100_JN.txt\n",
      "Reading claire_logout_d1_52.txt\n",
      "Reading claire_logout_d2_52.txt\n",
      "Reading logout1_50_RUSDI.txt\n",
      "Reading logout_50_JN.txt\n",
      "Reading logout_50_JN_2.txt\n",
      "Reading nic_logout_100.txt\n",
      "3228\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "print(len(X_train))\n",
    "\n",
    "#filePath = {1:'20201027/hair', 2:'20201027/rocket', 3:'20201027/zigzag', 4:'20201027/elbowLock',\n",
    "#            5:'20201027/pushBack',6:'20201027/scarecrow',7:'20201027/shoulder', 8:'20201027/window', 9:'20201027/logout'}\n",
    "filePath = {1:'20201111/hair', 2:'20201111/rocket', 3:'20201111/zigzag', 4:'20201111/elbowLock',\n",
    "            5:'20201111/pushBack',6:'20201111/scarecrow',7:'20201111/shoulder', 8:'20201111/window', 9:'20201111/logout'}\n",
    "\n",
    "\n",
    "for i in range(1,10):\n",
    "#for i in (1,2,5):   # hair = 1, rocket = 2, pushBack = 5\n",
    "    files = [f for f in os.listdir(filePath[i])]\n",
    "    for file in files:\n",
    "        print(\"Reading \" + file)\n",
    "        fullPath = filePath[i] + '/' + file\n",
    "        parse_data(fullPath, X_train, Y_train, i)\n",
    "# at this point, X_train and Y_train is filled up\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_train is 2928\n",
      "size of Y_train is 2928\n",
      "size of X_test is 300\n",
      "size of Y_test is 300\n"
     ]
    }
   ],
   "source": [
    "testSize = 300\n",
    "\n",
    "indexSet = set()\n",
    "while len(indexSet)<testSize:\n",
    "    indexSet.add(random.randint(0,len(X_train)-1))\n",
    "    \n",
    "indexList = list(indexSet)\n",
    "indexList.sort(reverse=True)\n",
    "\n",
    "for index in indexList:\n",
    "    X_test.append(X_train[index])\n",
    "    Y_test.append(Y_train[index])\n",
    "    X_train.pop(index)\n",
    "    Y_train.pop(index)\n",
    "    \n",
    "print(\"size of X_train is {}\".format(len(X_train)))\n",
    "print(\"size of Y_train is {}\".format(len(Y_train)))\n",
    "print(\"size of X_test is {}\".format(len(X_test)))\n",
    "print(\"size of Y_test is {}\".format(len(Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling inputs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(128, 256, 128), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       n_iter_no_change=30, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=10, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "layer1_size = 128\n",
    "layer2_size = 256\n",
    "layer3_size = 128\n",
    "max_iteration_size = 10000\n",
    "validation_fraction_size = 0.1\n",
    "n_iter_no_change_size = 30\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(layer1_size,layer2_size,layer3_size,),\n",
    "activation='logistic',\n",
    "max_iter=max_iteration_size,\n",
    "random_state=seed,\n",
    "solver='adam',\n",
    "shuffle=True,\n",
    "early_stopping=True,\n",
    "n_iter_no_change = n_iter_no_change_size,\n",
    "validation_fraction=validation_fraction_size)\n",
    "\n",
    "mlp.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "[9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 8 9 8 8 8 9 9 9 9 9 9 9 9 9 8 8 8\n",
      " 8 8 4 8 8 8 8 8 8 8 2 2 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 6 8 7 7 7 7 7\n",
      " 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 3 6 6 6 6 6 6 6 3 6 6 2\n",
      " 6 6 6 6 6 6 6 6 1 2 6 6 6 6 6 6 6 6 5 5 5 5 5 5 5 2 2 5 5 5 5 5 5 5 2 2 5\n",
      " 5 5 5 5 5 5 2 5 5 5 5 5 2 5 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 2 2 2 4 8 4 4 4 8 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 2 2 4 2 2 2 2 2 2 2 2 2 8 2 2 8 8 2 2 3 2 3 2 2 2 2 5 1 2 2 2 2 2 2\n",
      " 2 2 6 2 1 1 1 1 1 1 1 1 1 1 5 1 1 5 1 1 1 1 1 1 1 1 1 6 1 1 1 1 6 1 1 1 6\n",
      " 1 1 1 1]\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(set(Y_test))\n",
    "print(set(y_pred))\n",
    "print(y_pred)\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler1111_withoutUmar.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mlp, 'mlp20201111_withoutUmar.pkl', compress = 3)\n",
    "joblib.dump(scaler, 'scaler1111_withoutUmar.pkl', compress = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
