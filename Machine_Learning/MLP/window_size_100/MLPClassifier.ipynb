{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the function to parse the raw txt files and append to training and test buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(fileDirectory, X, Y, label):\n",
    "    source = open(fileDirectory, 'r')\n",
    "    data = source.readlines()\n",
    "    source.close()\n",
    "    for i in range(1, len(data)-1):\n",
    "        raw = data[i][1:len(data[i])-2]\n",
    "        processed = raw.split(\",\")\n",
    "        Y.append(label)\n",
    "        X.append([float(num) for num in processed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare the buffers for training and test data. Obtain the paths to all the data files and run parse_data to extract the raw data one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Reading claire_hair_r1_43.txt\n",
      "Reading claire_hair_r2_38.txt\n",
      "Reading claire_hair_r3_130.txt\n",
      "Reading hair4j.txt\n",
      "Reading hair5j.txt\n",
      "Reading hair6j.txt\n",
      "Reading hair7j.txt\n",
      "Reading hair8j.txt\n",
      "Reading hair_50_1.txt\n",
      "Reading hair_50_2.txt\n",
      "Reading hair_nic_1.txt\n",
      "Reading hair_nic_2.txt\n",
      "Reading umar_hair_1_50.txt\n",
      "Reading umar_hair_2_50.txt\n",
      "Reading umar_hair_3_50.txt\n",
      "Reading umar_hair_4_50.txt\n",
      "Reading claire_rocket_r1_23.txt\n",
      "Reading claire_rocket_r2_22.txt\n",
      "Reading claire_rocket_r3_23.txt\n",
      "Reading claire_rocket_r4_22.txt\n",
      "Reading claire_rocket_r5_22.txt\n",
      "Reading rocket1j.txt\n",
      "Reading rocket2j.txt\n",
      "Reading rocket3j.txt\n",
      "Reading rocket4j.txt\n",
      "Reading rocket5n.txt\n",
      "Reading rocket6n.txt\n",
      "Reading rocket7j.txt\n",
      "Reading rocket_20_rusdi_1.txt\n",
      "Reading rocket_20_rusdi_2.txt\n",
      "Reading rocket_20_rusdi_3.txt\n",
      "Reading rocket_20_rusdi_4.txt\n",
      "Reading rocket_20_rusdi_5.txt\n",
      "Reading rocket_nic_1.txt\n",
      "Reading rocket_nic_2.txt\n",
      "Reading rocket_nic_3.txt\n",
      "Reading rocket_nic_4.txt\n",
      "Reading umar_rocket_run1_50.txt\n",
      "Reading umar_rocket_run2_50.txt\n",
      "Reading umar_rocket_run3_30.txt\n",
      "Reading umar_rocket_run4_50.txt\n",
      "Reading umar_rocket_run_20.txt\n",
      "Reading claire_zigzag_r1_22.txt\n",
      "Reading claire_zigzag_r2_22.txt\n",
      "Reading claire_zigzag_r3_32.txt\n",
      "Reading claire_zigzag_r4_32.txt\n",
      "Reading jiannan_zigzag_h1_44.txt\n",
      "Reading jiannan_zigzag_h2_12.txt\n",
      "Reading nic_zigzag_73.txt\n",
      "Reading umar_zigzag_1_10.txt\n",
      "Reading umar_zigzag_1_5.txt\n",
      "Reading umar_zigzag_2_20.txt\n",
      "Reading umar_zigzag_2_20_1.txt\n",
      "Reading umar_zigzag_3_20.txt\n",
      "Reading umar_zigzag_3_20_1.txt\n",
      "Reading umar_zigzag_4_11.txt\n",
      "Reading umar_zigzag_5_50.txt\n",
      "Reading umar_zigzag_6_50.txt\n",
      "Reading umar_zigzag_7_50.txt\n",
      "Reading zigzag4j.txt\n",
      "Reading zigzagRusdi50.txt\n",
      "Reading zigzag_15_rusdi_1.txt\n",
      "Reading zigzag_15_rusdi_2.txt\n",
      "Reading zigzag_15_rusdi_3.txt\n",
      "Reading zigzag_15_rusdi_4.txt\n",
      "Reading zigzag_20_rusdi_1.txt\n",
      "Reading zigzag_20_rusdi_2.txt\n",
      "Reading zigzag_nic_1.txt\n",
      "Reading zigzag_nic_2.txt\n",
      "Reading zigzag_nic_3.txt\n",
      "Reading zigzag_nic_4.txt\n",
      "Reading zigzag_nic_5.txt\n",
      "Reading zigzag_nic_6.txt\n",
      "Reading zigzag_nic_7.txt\n",
      "Reading zigzag_nic_8.txt\n",
      "Reading zizzag3j.txt\n",
      "Reading claire_elbowlock_r1_82.txt\n",
      "Reading claire_elbowlock_r2_133.txt\n",
      "Reading elbowLock_JN_1_30.txt\n",
      "Reading elbowLock_JN_2_40.txt\n",
      "Reading elbowLock_JN_3_30.txt\n",
      "Reading elbowLock_JN_4_30.txt\n",
      "Reading elbowLock_JN_5_50.txt\n",
      "Reading elbowLock_JN_6_20.txt\n",
      "Reading elbowRusdi.txt\n",
      "Reading nic_elbowlock_1_55.txt\n",
      "Reading nic_elbowlock_2_165.txt\n",
      "Reading umar_elbowlock_1_50.txt\n",
      "Reading umar_elbowlock_2_50.txt\n",
      "Reading umar_elbowlock_3_50.txt\n",
      "Reading umar_elbowlock_4_50.txt\n",
      "Reading claire_pushback_r1_22.txt\n",
      "Reading claire_pushback_r2_26.txt\n",
      "Reading claire_pushback_r3_23.txt\n",
      "Reading claire_pushback_r4_150.txt\n",
      "Reading nic_pushback_205.txt\n",
      "Reading pushbackRusdi.txt\n",
      "Reading pushBack_JN_1_20.txt\n",
      "Reading pushBack_JN_2_20.txt\n",
      "Reading pushBack_JN_3_20.txt\n",
      "Reading pushBack_JN_4_30.txt\n",
      "Reading pushBack_JN_5_30.txt\n",
      "Reading pushBack_JN_6_55.txt\n",
      "Reading pushBack_JN_7_25.txt\n",
      "Reading umar_pusback_4_50.txt\n",
      "Reading umar_pushback_1_50.txt\n",
      "Reading umar_pushback_2_15.txt\n",
      "Reading umar_pushback_3_50.txt\n",
      "Reading umar_pushback_5_20.txt\n",
      "Reading umar_pushback_6_20.txt\n",
      "Reading claire_scarecrow_r1_22.txt\n",
      "Reading claire_scarecrow_r2_22.txt\n",
      "Reading claire_scarecrow_r3_22.txt\n",
      "Reading claire_scarecrow_r4_22.txt\n",
      "Reading claire_scarecrow_r5_22.txt\n",
      "Reading nic_scarecrow_1_50.txt\n",
      "Reading nic_scarecrow_2_55.txt\n",
      "Reading nic_scarecrow_3_55.txt\n",
      "Reading nic_scarecrow_4_56.txt\n",
      "Reading scarecrowRusdi.txt\n",
      "Reading scarecrow_JN_10_2.txt\n",
      "Reading scarecrow_JN_20_1.txt\n",
      "Reading scarecrow_JN_20_3.txt\n",
      "Reading scarecrow_JN_20_4.txt\n",
      "Reading scarecrow_JN_20_5.txt\n",
      "Reading scarecrow_JN_20_6.txt\n",
      "Reading scarecrow_JN_20_7.txt\n",
      "Reading scarecrow_JN_20_8.txt\n",
      "Reading scarecrow_JN_20_9.txt\n",
      "Reading scarecrow_JN_30_10.txt\n",
      "Reading umar_scarecrow_1_10.txt\n",
      "Reading umar_scarecrow_2_10.txt\n",
      "Reading umar_scarecrow_3_40.txt\n",
      "Reading umar_scarecrow_4_10.txt\n",
      "Reading umar_scarecrow_5_50.txt\n",
      "Reading umar_scarecrow_6_40.txt\n",
      "Reading umar_scarecrow_7_40.txt\n",
      "Reading claire_shouldershrug_r1_22.txt\n",
      "Reading claire_shouldershrug_r2_102.txt\n",
      "Reading nic_shouldershrug_211.txt\n",
      "Reading shoulderRusdi.txt\n",
      "Reading shoulder_JN_25_1.txt\n",
      "Reading shoulder_JN_40_3.txt\n",
      "Reading shoulder_JN_50_2.txt\n",
      "Reading shoulder_JN_60_2.txt\n",
      "Reading shoulder_JN_75_4.txt\n",
      "Reading umar_shouldershrug_1_50.txt\n",
      "Reading umar_shouldershrug_2_50.txt\n",
      "Reading umar_shouldershrug_3_50.txt\n",
      "Reading umar_shouldershrug_4_50.txt\n",
      "Reading claire_windowwipe_r1_22.txt\n",
      "Reading claire_windowwipe_r2_29.txt\n",
      "Reading claire_windowwipe_r3_41.txt\n",
      "Reading claire_windowwipe_r4_22.txt\n",
      "Reading umar_windows_3_50.txt\n",
      "Reading umar_window_1_30.txt\n",
      "Reading umar_window_2_20.txt\n",
      "Reading umar_window_4_50.txt\n",
      "Reading umar_window_5_50.txt\n",
      "Reading windowRusdi.txt\n",
      "Reading window_JN_25.txt\n",
      "Reading window_JN_40_1.txt\n",
      "Reading window_JN_50_3.txt\n",
      "Reading window_Nic_155_1.txt\n",
      "Reading claire_logout_r1_102.txt\n",
      "Reading logoutRusdi.txt\n",
      "Reading Logout_Nic_100_1.txt\n",
      "Reading Special_JN_20_2.txt\n",
      "Reading special_JN_20_3.txt\n",
      "Reading Special_JN_21_1.txt\n",
      "Reading special_JN_50_4.txt\n",
      "Reading umar_logout_1_50.txt\n",
      "Reading umar_logout_2_20.txt\n",
      "Reading umar_logout_3_20.txt\n",
      "Reading umar_logout_4_10.txt\n",
      "7863\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "print(len(X_train))\n",
    "\n",
    "filePath = {1:'20201027/hair', 2:'20201027/rocket', 3:'20201027/zigzag', 4:'20201027/elbowLock',\n",
    "            5:'20201027/pushBack',6:'20201027/scarecrow',7:'20201027/shoulder', 8:'20201027/window', 9:'20201027/logout'}\n",
    "for i in range(1,10):  # hair = 1, rocket = 2, zigzag = 3\n",
    "    files = [f for f in os.listdir(filePath[i])]\n",
    "    for file in files:\n",
    "        print(\"Reading \" + file)\n",
    "        fullPath = filePath[i] + '/' + file\n",
    "        parse_data(fullPath, X_train, Y_train, i)\n",
    "# at this point, X_train and Y_train is filled up\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This step is used for randomly spliting the whole datasets into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_train is 7062\n",
      "size of Y_train is 7062\n",
      "size of X_test is 801\n",
      "size of Y_test is 801\n"
     ]
    }
   ],
   "source": [
    "testSize = 800\n",
    "\n",
    "indexSet = set()\n",
    "while len(indexSet)<=testSize:\n",
    "    indexSet.add(random.randint(0,len(X_train)-1))\n",
    "    \n",
    "indexList = list(indexSet)\n",
    "indexList.sort(reverse=True)\n",
    "\n",
    "for index in indexList:\n",
    "    X_test.append(X_train[index])\n",
    "    Y_test.append(Y_train[index])\n",
    "    X_train.pop(index)\n",
    "    Y_train.pop(index)\n",
    "    \n",
    "print(\"size of X_train is {}\".format(len(X_train)))\n",
    "print(\"size of Y_train is {}\".format(len(Y_train)))\n",
    "print(\"size of X_test is {}\".format(len(X_test)))\n",
    "print(\"size of Y_test is {}\".format(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the datasets using Standard Scaler from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling inputs\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "#X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train the MLP Model from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
       "       beta_1=0.9, beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(250, 250, 250), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=10000, momentum=0.9,\n",
       "       n_iter_no_change=30, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=10, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 10\n",
    "layer1_size = 250\n",
    "layer2_size = 250\n",
    "layer3_size = 250\n",
    "max_iteration_size = 10000\n",
    "validation_fraction_size = 0.1\n",
    "n_iter_no_change_size = 30\n",
    "\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(layer1_size,layer2_size,layer3_size,),\n",
    "activation='logistic',\n",
    "max_iter=max_iteration_size,\n",
    "random_state=seed,\n",
    "solver='adam',\n",
    "shuffle=True,\n",
    "early_stopping=True,\n",
    "n_iter_no_change = n_iter_no_change_size,\n",
    "validation_fraction=validation_fraction_size)\n",
    "\n",
    "mlp.fit(X_train_scaled, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This step is to run the model on our test dataset and see the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916354556803995\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "[9 3 9 9 9 9 9 9 4 3 9 9 9 9 9 9 9 9 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 8 8 8 8 8 8 8 8 8 8 8 5 8 4 8 8 8 7 8 8 7 8 8 8 8\n",
      " 8 8 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 5 8 9]\n",
      "[9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "y_pred = mlp.predict(X_test_scaled)\n",
    "print(accuracy_score(Y_test,y_pred))\n",
    "print(set(Y_test))\n",
    "print(set(y_pred))\n",
    "print(y_pred[:100])\n",
    "print(Y_test[:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This step creates the python pickle files for scaler and MLP model respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler1112v1.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mlp, 'mlp20201112v1.pkl', compress = 3)\n",
    "joblib.dump(scaler, 'scaler1112v1.pkl', compress = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
